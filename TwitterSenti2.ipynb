{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.679000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ColeyGirouard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir I am scheduled for the morning, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17-02-2015 20:16</td>\n",
       "      <td>Washington D.C.</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.699890e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WalterFaddoul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir seeing your workers time in and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23-02-2015 14:36</td>\n",
       "      <td>Indianapolis, Indiana; USA</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.680890e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LocalKyle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united Flew ORD to Miami and back and  had gr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18-02-2015 08:46</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.689280e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>amccarthy19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir @dultch97 that's horse radish üò§üê¥</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20-02-2015 16:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.685940e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J_Okayy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united so our flight into ORD was delayed bec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19-02-2015 18:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10975</td>\n",
       "      <td>5.699340e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cottopanama85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir followback</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23-02-2015 10:58</td>\n",
       "      <td>ohio,panama</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10976</td>\n",
       "      <td>5.685640e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PaulBEsteves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united thanks for the help. Wish the phone re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19-02-2015 16:13</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10977</td>\n",
       "      <td>5.696440e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>runfixsteve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@usairways the. Worst. Ever. #dca #customerser...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22-02-2015 15:43</td>\n",
       "      <td>St. Augustine, Florida</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10978</td>\n",
       "      <td>5.688650e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLChicosky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@nrhodes85: look! Another apology. DO NOT FLY ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20-02-2015 12:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10979</td>\n",
       "      <td>5.689290e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JW_Blocker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>@united you are by far the worst airline. 4 pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20-02-2015 16:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10980 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline airline_sentiment_gold  \\\n",
       "0      5.679000e+17          negative   Southwest                    NaN   \n",
       "1      5.699890e+17          positive   Southwest                    NaN   \n",
       "2      5.680890e+17          positive      United                    NaN   \n",
       "3      5.689280e+17          negative   Southwest                    NaN   \n",
       "4      5.685940e+17          negative      United                    NaN   \n",
       "...             ...               ...         ...                    ...   \n",
       "10975  5.699340e+17           neutral    American                    NaN   \n",
       "10976  5.685640e+17          positive      United                    NaN   \n",
       "10977  5.696440e+17          negative  US Airways                    NaN   \n",
       "10978  5.688650e+17          negative  US Airways                    NaN   \n",
       "10979  5.689290e+17          negative      United                    NaN   \n",
       "\n",
       "                name negativereason_gold  retweet_count  \\\n",
       "0      ColeyGirouard                 NaN              0   \n",
       "1      WalterFaddoul                 NaN              0   \n",
       "2          LocalKyle                 NaN              0   \n",
       "3        amccarthy19                 NaN              0   \n",
       "4            J_Okayy                 NaN              0   \n",
       "...              ...                 ...            ...   \n",
       "10975  Cottopanama85                 NaN              0   \n",
       "10976   PaulBEsteves                 NaN              0   \n",
       "10977    runfixsteve                 NaN              0   \n",
       "10978     CLChicosky                 NaN              0   \n",
       "10979     JW_Blocker                 NaN              1   \n",
       "\n",
       "                                                    text tweet_coord  \\\n",
       "0      @SouthwestAir I am scheduled for the morning, ...         NaN   \n",
       "1      @SouthwestAir seeing your workers time in and ...         NaN   \n",
       "2      @united Flew ORD to Miami and back and  had gr...         NaN   \n",
       "3         @SouthwestAir @dultch97 that's horse radish üò§üê¥         NaN   \n",
       "4      @united so our flight into ORD was delayed bec...         NaN   \n",
       "...                                                  ...         ...   \n",
       "10975                            @AmericanAir followback         NaN   \n",
       "10976  @united thanks for the help. Wish the phone re...         NaN   \n",
       "10977  @usairways the. Worst. Ever. #dca #customerser...         NaN   \n",
       "10978  @nrhodes85: look! Another apology. DO NOT FLY ...         NaN   \n",
       "10979  @united you are by far the worst airline. 4 pl...         NaN   \n",
       "\n",
       "          tweet_created              tweet_location  \\\n",
       "0      17-02-2015 20:16             Washington D.C.   \n",
       "1      23-02-2015 14:36  Indianapolis, Indiana; USA   \n",
       "2      18-02-2015 08:46                    Illinois   \n",
       "3      20-02-2015 16:20                         NaN   \n",
       "4      19-02-2015 18:13                         NaN   \n",
       "...                 ...                         ...   \n",
       "10975  23-02-2015 10:58                 ohio,panama   \n",
       "10976  19-02-2015 16:13                    Brooklyn   \n",
       "10977  22-02-2015 15:43      St. Augustine, Florida   \n",
       "10978  20-02-2015 12:09                         NaN   \n",
       "10979  20-02-2015 16:24                         NaN   \n",
       "\n",
       "                    user_timezone  \n",
       "0          Atlantic Time (Canada)  \n",
       "1      Central Time (US & Canada)  \n",
       "2      Central Time (US & Canada)  \n",
       "3          Atlantic Time (Canada)  \n",
       "4      Eastern Time (US & Canada)  \n",
       "...                           ...  \n",
       "10975                         NaN  \n",
       "10976  Eastern Time (US & Canada)  \n",
       "10977                         NaN  \n",
       "10978                         NaN  \n",
       "10979                         NaN  \n",
       "\n",
       "[10980 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.read_csv(\"twitter_train.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data['negativereason_gold']\n",
    "del train_data['airline_sentiment_gold']\n",
    "del train_data['tweet_coord']\n",
    "del train_data['tweet_id']\n",
    "del train_data['name']\n",
    "del train_data['tweet_created']\n",
    "del train_data['user_timezone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting string values to numbers in airline column\n",
    "airline_num={'Southwest':1, 'US Airways':2, 'Delta':3, 'American':4, 'United':5, 'Virgin America':6}\n",
    "train_data['airline']=[airline_num[item] for item in train_data['airline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data['tweet_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating doc in the format:- [[comments],[sentiment type]]\n",
    "train_docs=()\n",
    "for i in range(len(train_data)):\n",
    "    train_docs.append((train_data['text'][i],train_data['airline_sentiment'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning text in docs:-\n",
    "\n",
    "#(1)Creatting set of stopwords and punctuations\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stops=set(stopwords.words('english'))\n",
    "punctuations=list(string.punctuation)\n",
    "stops.update(punctuations)\n",
    "\n",
    "#(2)Function to get simple pos form from WordNet dataset\n",
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#(3)Importing Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#(4)Tokenizing function\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for i in range(len(train_docs)):\n",
    "    train_docs[i][0]=word_tokenize(str(train_docs[i][0]))\n",
    "    \n",
    "    \n",
    "#(5)Cleaning Function\n",
    "from nltk import pos_tag\n",
    "def clean(words):\n",
    "    output_words=[]\n",
    "    for w in words:\n",
    "        w=w.lower()\n",
    "        if w not in stops:\n",
    "            pos=pos_tag([w])\n",
    "            cleaned_word=lemmatizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
    "            output_words.append(cleaned_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_docs)):\n",
    "    train_docs[i][0]=clean(train_docs[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating combined worset\n",
    "allWords=[]\n",
    "for i in range(len(train_docs)):\n",
    "    allWords+=train_docs[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting frequency distribution for all words\n",
    "import nltk\n",
    "freq=nltk.FreqDist(allWords) #Storing the object obtained in another object called freq\n",
    "common=freq.most_common(3000) \n",
    "#Array of tuples obtained\n",
    "features= [i[0] for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary with words and whether they are present or not\n",
    "def get_feature_dict(words):\n",
    "    current_features={}\n",
    "    wordSet=set(words)\n",
    "    for w in features:\n",
    "        current_features[w]= w in wordSet\n",
    "    return current_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=get_feature_dict(train_docs[0][0])\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=[]\n",
    "for i in range(len(train_docs)):\n",
    "    x_train.append((get_feature_dict(train_docs[i][0]), train_docs[i][1]))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "classifier=NaiveBayesClassifier.train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
